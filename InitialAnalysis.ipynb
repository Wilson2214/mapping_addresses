{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35fa44bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial analysis of data and layout of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eeeaed4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from io import BytesIO\n",
    "from zipfile import ZipFile\n",
    "from urllib.request import urlopen\n",
    "import requests\n",
    "import os\n",
    "import gzip\n",
    "import shutil\n",
    "import gdown\n",
    "import subprocess\n",
    "from math import radians, cos, sin, asin, sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfec370d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "731cecb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gh/2jk0_tgj2s5czzj043hknhb40000gn/T/ipykernel_29085/2977275755.py:4: DtypeWarning: Columns (11,12,14,20) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  locations = pd.read_csv(reconstructed_locations_url)\n"
     ]
    }
   ],
   "source": [
    "# Get Locations URL and Load into Pandas Dataframe\n",
    "locations_url = \"https://drive.google.com/file/d/1WOmj8wSpe8FDn_7opryh9tsng8ZqhAr1/view?usp=share_link\"\n",
    "reconstructed_locations_url = 'https://drive.google.com/uc?id=' + locations_url.split('/')[-2]\n",
    "locations = pd.read_csv(reconstructed_locations_url)\n",
    "\n",
    "cwd = os.getcwd()\n",
    "locations.to_csv(cwd +'/data/locations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1781d28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "807b2d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of locations provided: 156900\n",
      "Number of locations with missing f_lat value: 7718\n",
      "Number of locations with missing f_lon value: 7718\n"
     ]
    }
   ],
   "source": [
    "# Cleaning Steps\n",
    "# Remove Rows without f_lat & f_lon\n",
    "\n",
    "# Initial research shows that these addresses are likely related to land without a building\n",
    "# 200 Wesley Ave, 3963 Morrison Rd, etc. seem to be vacant land or lots\n",
    "\n",
    "print('Original number of locations provided: ' + str(locations.shape[0]))\n",
    "print('Number of locations with missing f_lat value: ' + str(locations['f_lat'].isna().sum()))\n",
    "print('Number of locations with missing f_lon value: ' + str(locations['f_lon'].isna().sum()))\n",
    "\n",
    "#locations = locations.dropna(subset=['f_lat','f_lon'])\n",
    "\n",
    "#print('Total Number of locations after dropping NaN lat/long values: ' + str(locations.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "8915b6ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114254"
      ]
     },
     "execution_count": 418,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(locations['parcel_id'] != 'None')\n",
    "# Potentially 114254 or 68% could be matched directly to a building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "141753b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get buildings data\n",
    "# https://stackoverflow.com/questions/5710867/downloading-and-unzipping-a-zip-file-without-writing-to-disk\n",
    "buildings_url = \"https://drive.google.com/file/d/1qO86txHm82OqWbEEIEsaevF_tRFv4PhK/view?usp=share_link\"\n",
    "reconstructed_buildings_url = 'https://drive.google.com/uc?id=' + buildings_url.split('/')[-2]\n",
    "\n",
    "cwd = os.getcwd()\n",
    "\n",
    "resp = urlopen(reconstructed_buildings_url)\n",
    "myzip = ZipFile(BytesIO(resp.read()))\n",
    "myzip.extractall(cwd +'/data/')\n",
    "\n",
    "# Read json\n",
    "buildings = gpd.read_file(cwd +'/data/ms_hinds_buildings.json')\n",
    "buildings_join_tbl = pd.read_csv(cwd +'/data/ms_hinds_buildings_join_table.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce983cc5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(buildings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b7f8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get parcels data\n",
    "parcels_url = \"https://drive.google.com/file/d/137bCzf0qEPLwKTgNxeAAjTs5MvyTlxWA/view?usp=share_link\"\n",
    "reconstructed_parcels_url = 'https://drive.google.com/uc?id=' + parcels_url.split('/')[-2]\n",
    "\n",
    "# Use gdown to download large gz file: https://pypi.org/project/gdown/\n",
    "output = cwd + '/data/ms_hinds_parcels.ndgeojson.gz'\n",
    "gdown.download(reconstructed_parcels_url, output, quiet=False)\n",
    "\n",
    "# Extract and save gzip file\n",
    "gzip_filename = cwd + '/data/ms_hinds_parcels.ndgeojson.gz'\n",
    "dest_filepath = cwd + '/data/ms_hinds_parcels.ndgeojson'\n",
    "with gzip.open(gzip_filename, 'rb') as file:\n",
    "        with open(dest_filepath, 'wb') as output_file:\n",
    "            output_file.write(file.read())\n",
    "\n",
    "parcels = gpd.read_file(cwd +'/data/ms_hinds_parcels.ndgeojson')   \n",
    "\n",
    "# Method not needed: use geopandas to directly load file\n",
    "# Original method was to use shell script and convert to geojson\n",
    "# Use shell script to convert npgeojson to geojson\n",
    "#subprocess.call(['sh', './convert_json.sh'])\n",
    "\n",
    "# # Read json\n",
    "# final_filepath = cwd + '/data/ms_hinds_parcels.geojson'\n",
    "# parcels1 = gpd.read_file(final_filepath)\n",
    "\n",
    "# # New method read as json, then normalize\n",
    "# parcels = pd.read_json(cwd + '/data/ms_hinds_parcels.ndgeojson', lines=True)\n",
    "# parcels = pd.json_normalize(parcels.properties)\n",
    "# parcels.to_csv(cwd +'/data/parcels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "554cef0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gh/2jk0_tgj2s5czzj043hknhb40000gn/T/ipykernel_34714/1695776075.py:3: DtypeWarning: Columns (11,12,14,20) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  locations = pd.read_csv(cwd +'/data/locations.csv')\n"
     ]
    }
   ],
   "source": [
    "# # Quick to test re-run\n",
    "# cwd = os.getcwd()\n",
    "# locations = pd.read_csv(cwd +'/data/locations.csv')\n",
    "# buildings = gpd.read_file(cwd +'/data/ms_hinds_buildings.json')\n",
    "# buildings_join_tbl = pd.read_csv(cwd +'/data/ms_hinds_buildings_join_table.csv')\n",
    "# parcels = gpd.read_file(cwd +'/data/ms_hinds_parcels.ndgeojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e790087b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gh/2jk0_tgj2s5czzj043hknhb40000gn/T/ipykernel_34714/1114220313.py:5: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  parcels_join['szip'] = parcels_join['szip'].str.replace(\"-.*\",\"\")\n",
      "/var/folders/gh/2jk0_tgj2s5czzj043hknhb40000gn/T/ipykernel_34714/1114220313.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  parcels_join['szip'] = parcels_join['szip'].str.replace(\"-.*\",\"\")\n"
     ]
    }
   ],
   "source": [
    "# Build combined parcel/bldg table\n",
    "# Isolate from parcel: ll_uuid, address, szip\n",
    "parcels_join = parcels[['ll_uuid', 'address', 'szip']]\n",
    "# Confirm szip is 5 digit zip code to match f_ziplock\n",
    "parcels_join['szip'] = parcels_join['szip'].str.replace(\"-.*\",\"\")\n",
    "# Confirmed there were no duplicates here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "774f282c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join to get ed_str_uuid and ed_bld_uuid\n",
    "buildings_join_tbl = buildings_join_tbl.drop('geoid', axis=1)\n",
    "parcels_join = parcels_join.merge(buildings_join_tbl, on='ll_uuid', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55bc3b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all ll_uuid without a str and bld uuid\n",
    "parcels_join = parcels_join.dropna(subset=['ed_str_uuid','ed_bld_uuid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "879bbe07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join to buildings on both uuids to get ed_lat/ed_lon\n",
    "buildings_join = buildings[['ed_str_uuid', 'ed_bld_uuid', 'ed_lat', 'ed_lon']]\n",
    "parcels_join = parcels_join.merge(buildings_join, on=['ed_str_uuid', 'ed_bld_uuid'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f5384025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined Function\n",
    "# Each location has a lamda function run against it to assign the best possible lat/long and distance from original\n",
    "# 1. If location has a parcel_id and only one building on the parcel, the location will be assigned the building's lat/long\n",
    "# 2. If location has a parcel_id and there are multiple buildings, calculate the smallest distance and assign that building's lat/long\n",
    "# 3. If location has a parcel_id, but no point lat/long, assign the first building id associated with the parcel\n",
    "# 4. If location has no parcel_id, look at nearest buildings and assign the closest building lat/long with the parcel\n",
    "# 5. If location has no parcel_id or f_lat/f_long, no changes will be made\n",
    "\n",
    "\n",
    "def hav_dist(lat1, long1, lat2, long2):\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance between two points \n",
    "    on the earth (specified in decimal degrees)\n",
    "    \"\"\"\n",
    "    # convert decimal degrees to radians \n",
    "    lat1, long1, lat2, long2 = map(radians, [lat1, long1, lat2, long2])\n",
    "    # haversine formula \n",
    "    dlon = long2 - long1 \n",
    "    dlat = lat2 - lat1 \n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * asin(sqrt(a)) \n",
    "    # Radius of earth in kilometers is 6371\n",
    "    km = 6371* c\n",
    "    return km\n",
    "\n",
    "# def euc_dist(lat1, long1, lat2, long2):\n",
    "#     return(np.sqrt(np.power(abs(lat1-lat2),2)+np.power(abs(long1-long2),2)))\n",
    "\n",
    "def find_nearest(lat, lon, parcels_join_filt):\n",
    "    distances = parcels_join_filt.apply(\n",
    "        lambda row: hav_dist(lat, lon, row['ed_lat'], row['ed_lon']), \n",
    "        axis=1)\n",
    "    return (parcels_join_filt.loc[distances.idxmin()]['ed_lat'], \n",
    "            parcels_join_filt.loc[distances.idxmin()]['ed_lon'],\n",
    "            distances.min())\n",
    "\n",
    "def get_updated_geo(ll_uuid,lat,lon):\n",
    "    \n",
    "    # parcels_join is loaded previously\n",
    "    # Filter parcels_join for all buildings associated with parsel\n",
    "    parcels_join_filt = parcels_join[parcels_join['ll_uuid'] == ll_uuid]\n",
    "    \n",
    "    if parcels_join_filt.shape[0] == 0:\n",
    "        # Calculate closest building\n",
    "        if np.isnan(lat):\n",
    "            return(None)\n",
    "        else:\n",
    "            # Filter buildings lat long for different ranges to make for a smaller subset\n",
    "            close_buildings = buildings[(buildings['ed_lat'] >= lat - 0.001) & \n",
    "                                        (buildings['ed_lat'] <= lat + 0.001) & \n",
    "                                        (buildings['ed_lon'] >= lon - 0.001) & \n",
    "                                        (buildings['ed_lon'] <= lon + 0.001)]\n",
    "            if close_buildings.shape[0] == 0:\n",
    "                # Refilter for a larger subset\n",
    "                close_buildings = buildings[(buildings['ed_lat'] >= lat - 0.01) & \n",
    "                                            (buildings['ed_lat'] <= lat + 0.01) & \n",
    "                                            (buildings['ed_lon'] >= lon - 0.01) & \n",
    "                                            (buildings['ed_lon'] <= lon + 0.01)]\n",
    "            if close_buildings.shape[0] == 0:\n",
    "                # Refilter for a larger subset\n",
    "                close_buildings = buildings[(buildings['ed_lat'] >= lat - 0.1) & \n",
    "                                            (buildings['ed_lat'] <= lat + 0.1) & \n",
    "                                            (buildings['ed_lon'] >= lon - 0.1) & \n",
    "                                            (buildings['ed_lon'] <= lon + 0.1)]\n",
    "            if close_buildings.shape[0] == 0:\n",
    "                # Refilter for a larger subset\n",
    "                close_buildings = buildings[(buildings['ed_lat'] >= lat - 1.0) & \n",
    "                                            (buildings['ed_lat'] <= lat + 1.0) & \n",
    "                                            (buildings['ed_lon'] >= lon - 1.0) & \n",
    "                                            (buildings['ed_lon'] <= lon + 1.0)]\n",
    "            if close_buildings.shape[0] == 0:\n",
    "                return(None)\n",
    "            else:\n",
    "                return(find_nearest(lat,lon,close_buildings))\n",
    "        \n",
    "    elif parcels_join_filt.shape[0] == 1:\n",
    "        # There is only one building associated with this parsel\n",
    "        # Return lat/long of this building as the updated coordinates\n",
    "        if np.isnan(lat):\n",
    "            distance = 0\n",
    "        else:\n",
    "            distance = hav_dist(lat, lon, parcels_join_filt.ed_lat, parcels_join_filt.ed_lon)\n",
    "        return(parcels_join_filt.ed_lat.iloc[0], parcels_join_filt.ed_lon.iloc[0], distance)\n",
    "    \n",
    "    else:\n",
    "        # There are multiple buildings associated with this matching parsel\n",
    "        # Calculate the haversine distance between the location and each building\n",
    "        # Return the closest building\n",
    "        if np.isnan(lat):\n",
    "            parcels_join_filt = parcels_join_filt.groupby(\"ll_uuid\").first()\n",
    "            distance = 0\n",
    "            return(parcels_join_filt.ed_lat.iloc[0], parcels_join_filt.ed_lon.iloc[0], distance)\n",
    "        else:\n",
    "            return(find_nearest(lat,lon,parcels_join_filt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d0cbc0e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 27min 31s, sys: 30.1 s, total: 28min 1s\n",
      "Wall time: 29min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# This will run and take around 30 minutes\n",
    "buildings = buildings[['ed_str_uuid', 'ed_bld_uuid', 'ed_lat', 'ed_lon']]\n",
    "locations['updated_geo'] = locations.apply(lambda x: get_updated_geo(x.parcel_id, x.f_lat, x.f_lon), axis=1)\n",
    "locations.to_csv(cwd +'/data/locations_updated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d10decad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING\n",
    "# locations2 = locations.sample(n=1000)\n",
    "# buildings = buildings[['ed_str_uuid', 'ed_bld_uuid', 'ed_lat', 'ed_lon']]\n",
    "# locations2 = locations[locations['parcel_id'] == '6adfe635-0a7a-42ea-83ff-cda265bd077e']\n",
    "# locations2['updated_geo'] = locations2.apply(lambda x: get_updated_geo(x.parcel_id, x.f_lat, x.f_lon), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "2a814965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING\n",
    "# locations2.to_csv(cwd +'/data/locations_testing_sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e97b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "locations_updated = pd.read_csv(cwd +'/data/locations_updated.csv')\n",
    "\n",
    "# Split and clean updated information\n",
    "locations_updated['updated_lat'] = locations_updated['updated_geo'].str.split(',', expand = True)[0]\n",
    "locations_updated['updated_lon'] = locations_updated['updated_geo'].str.split(',', expand = True)[1]\n",
    "locations_updated['distance_km'] = locations_updated['updated_geo'].str.split(',', expand = True)[2]\n",
    "\n",
    "locations_updated['updated_lat'] = locations_updated['updated_lat'].str.replace(\"(\",\"\")\n",
    "locations_updated['distance_km'] = locations_updated['distance_km'].str.replace(\")\",\"\")\n",
    "\n",
    "locations_updated['updated_lat'] = pd.to_numeric(locations_updated['updated_lat'])\n",
    "locations_updated['updated_lon'] = pd.to_numeric(locations_updated['updated_lon'])\n",
    "locations_updated['distance_km'] = pd.to_numeric(locations_updated['distance_km'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7054ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deliverable 1: Data table: with improved geo-precision of address locations, \n",
    "# ideally matching each address to the correct building with the address’s coords updated to building rooftop.\n",
    "\n",
    "# Save Locations updated with new lat/long/distance from original point\n",
    "locations_updated.to_csv(cwd +'/data/final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db99a5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deliverable 2: Visualization of locations BEFORE v.s. AFTER on top of building polygons / parcel polygons\n",
    "# See other file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c208323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deliverable 3: Metrics:\n",
    "# 1. On average, how far are original geolocation moved\n",
    "\n",
    "#locations_updated['distance_km'] = pd.to_numeric(locations_updated['distance_km'])\n",
    "avg_distance_moved = locations_updated['distance_km'].mean()\n",
    "min_distance_moved = locations_updated['distance_km'].min()\n",
    "max_distance_moved = locations_updated['distance_km'].max()\n",
    "\n",
    "# 2. How many points with too little information to move anywhere?\n",
    "# Only points without an original lat/long and a parcel without a matching building could not be moved\n",
    "total_locations = locations_updated.shape[0]\n",
    "change_locations = locations_updated.dropna(subset=['updated_geo']).shape[0]\n",
    "no_change_locations = total_locations - change_locations\n",
    "\n",
    "# 3. What else (other data) can we use to enhance the accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be090dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Average distance (km) points moved: ' + str(avg_distance_moved))\n",
    "print('Smallest distance (km) moved: ' + str(min_distance_moved))\n",
    "print('Largest distance (km) moved: ' + str(max_distance_moved))\n",
    "print('Number of Locations without an updated geolocation: ' + str(no_change_locations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a9f7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[avg_distance_moved, min_distance_moved, max_distance_moved, no_change_locations]]\n",
    "df = pd.DataFrame(data, columns=['avg_distance_moved', 'min_distance_moved', 'max_distance_moved', 'no_change_locations'])\n",
    "\n",
    "display(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo_env",
   "language": "python",
   "name": "geo_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
